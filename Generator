#!/usr/bin/python3

import xml.etree.ElementTree as ET
import pandas as pd
import json
import time
from random import seed
from random import gauss
from kafka import KafkaProducer
import numpy as np

source_files = ['Posts.xml', 'Comments.xml', 'Users.xml']    # użyte pliki z dostarczonych danych
path = f'~//'     # aktualna ścieżka plików źródłowych
df_input = [0, 0, 0]

xml_keys = [['Id', 'CreationDate', 'ViewCount', 'Body', 'OwnerUserId', 'Title', 'Tags'],
           ['Id', 'PostId', 'Score', 'Text', 'CreationDate', 'UserId', 'ContentLicense'],
           ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'Location', 'AboutMe', 'Views',
           'UpVotes', 'DownVotes', 'AccountId']]

for i, file in enumerate(source_files):   # parsowanie plików .xml
    tree = ET.parse(path+file)
    root = tree.getroot()
    current_keys = xml_keys[i]
    df_data = pd.DataFrame()
    for row in root:
        temp_dict = {}
        for key in row.attrib:
            if key in current_keys:
                temp_dict[key] = row.attrib[key]
        if len(temp_dict) > 0:
            df_data = df_data.append(temp_dict, ignore_index=True)
            df_input[i] = df_data

df_posts = df_input[0]
df_comments = df_input[1]
df_users = df_input[2]

df_posts['Date'] = pd.to_datetime(df_posts['CreationDate'], format="%Y-%m-%dT%H:%M:%S")
df_comments['Date'] = pd.to_datetime(df_comments['CreationDate'], format="%Y-%m-%dT%H:%M:%S")
df_users['Date'] = pd.to_datetime(df_users['CreationDate'], format="%Y-%m-%dT%H:%M:%S")

df_posts['Type'] = 1
df_comments['Type'] = 2
df_users['Type'] = 3

posts_list = df_posts.loc[:,['Date', 'Id', 'Type']]
comments_list = df_comments.loc[:,['Date', 'Id', 'Type']]
users_list = df_users.loc[:,['Date', 'Id', 'Type']]

df_posts['postid'] = df_posts['Id']
df_comments['commentid'] = df_comments['Id']
df_posts.set_index('Id', inplace=True)
df_comments.set_index('Id', inplace=True)
df_users.set_index('Id', inplace=True)

df_posts.drop('Date', axis=1, inplace=True)
df_comments.drop('Date', axis=1, inplace=True)
df_users.drop('Date', axis=1, inplace=True)

df_posts.replace(np.NaN, '0', inplace=True)
df_comments.replace(np.NaN, '0', inplace=True)
df_users.replace(np.NaN, '0', inplace=True)

generator_list = posts_list.append(comments_list).append(users_list)
generator_list.sort_values(by='Date', inplace=True)    # segregowanie danych w kolejności wystąpienia
generator_list.reset_index(drop=True, inplace=True)

n = 0.1     # wspołczynnik opóźnienia wysyłania zdarzeń
seed(1)

producer = KafkaProducer(bootstrap_servers=['172.31.75.133:9092'],     # wkleić Kafka private IP
                         value_serializer=lambda m: json.dumps(m).encode('utf-8'))

try:

    for i in range(len(generator_list)):     # wszytskie pozycje w danych źródłowych
        delay = abs(gauss(0, 2)) * n
        time.sleep(delay)

        Type = generator_list['Type'].iloc[i]
        ID = generator_list['Id'].iloc[i]

        if Type == 1:
            output_data = {"postid": df_posts.loc[ID, 'postid'], "Body": df_posts.loc[ID, 'Body'],
                           "CreationDate": df_posts.loc[ID, 'CreationDate'],
                           "OwnerUserId": df_posts.loc[ID, 'OwnerUserId'], "Tags": df_posts.loc[ID, 'Tags'],
                           "Title": df_posts.loc[ID, 'Title'], "ViewCount": df_posts.loc[ID, 'ViewCount'],
                           "Type": str(df_posts.loc[ID, 'Type'])}
            topic = 'post'

        elif Type == 2:
            output_data = {"commentid": df_comments.loc[ID, 'commentid'],
                           "ContentLicense": df_comments.loc[ID, 'ContentLicense'],
                           "CreationDate": df_comments.loc[ID, "CreationDate"], "PostId": df_comments.loc[ID, "PostId"],
                           "Score": df_comments.loc[ID, 'Score'], "Text": df_comments.loc[ID, 'Text'],
                           "UserId": df_comments.loc[ID, 'UserId'], "Type": str(df_comments.loc[ID, 'Type'])}
            topic = 'answer'

        elif Type == 3:
            output_data = {"AccountId": df_users.loc[ID, 'AccountId'], "AboutMe": df_users.loc[ID, 'AboutMe'],
                           "CreationDate": df_users.loc[ID, 'CreationDate'], "DisplayName": df_users.loc[ID, 'DisplayName'],
                           "DownVotes": df_users.loc[ID, 'DownVotes'], "LastAccessDate": df_users.loc[ID, 'LastAccessDate'],
                           "Location": df_users.loc[ID, 'Location'], "Reputation": df_users.loc[ID, 'Reputation'],
                           "UpVotes": df_users.loc[ID, 'UpVotes'], "Views": df_users.loc[ID, 'Views'],
                           "Type": str(df_users.loc[ID, 'Type'])}
            topic='user'

        else:
            continue

        producer.send(topic, value=output_data)
        print('sending to Kafka...')
        print(output_data, '\n')

except KeyboardInterrupt:
    producer.close()
