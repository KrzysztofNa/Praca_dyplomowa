from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import re

kafka_servers = 'localhost:9092'
appName = "3dprinting"
master = "local"

jsonSchema_post = StructType([StructField("postid", StringType(), True),
                              StructField("Body", StringType(), True),
                              StructField("CreationDate", StringType(), True),
                              StructField("OwnerUserId", StringType(), True),
                              StructField("Tags", StringType(), True),
                              StructField("Title", StringType(), True),
                              StructField("ViewCount", StringType(), True),
                              StructField("Type", StringType(), True)])

jsonSchema_answer = StructType([StructField("commentid", StringType(), True),
                                StructField("ContentLicense", StringType(), True),
                                StructField("CreationDate", StringType(), True),
                                StructField("PostId", StringType(), True),
                                StructField("Score", StringType(), True),
                                StructField("Text", StringType(), True),
                                StructField("UserId", StringType(), True),
                                StructField("Type", StringType(), True)])

jsonSchema_user = StructType([StructField("AccountId", StringType(), True),
                              StructField("AboutMe", StringType(), True),
                              StructField("CreationDate", StringType(), True),
                              StructField("DisplayName", StringType(), True),
                              StructField("DownVotes", StringType(), True),
                              StructField("LastAccessDate", StringType(), True),
                              StructField("Location", StringType(), True),
                              StructField("Reputation", StringType(), True),
                              StructField("UpVotes", StringType(), True),
                              StructField("Views", StringType(), True),
                              StructField("Type", StringType(), True)])

spark = SparkSession.builder \
    .master(master) \
    .getOrCreate()

stream = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_servers) \
    .option("subscribe", "post, answer, user") \
    .option("startingOffsets", "earliest") \
    .load()

data = stream.selectExpr("CAST(value AS STRING)", "topic")

data_post = data.filter(col('topic').isin(['post']))
data_answer = data.filter(col('topic').isin(['answer']))
data_user = data.filter(col('topic').isin(['user']))

data_post1 = data_post.withColumn("value", from_json("value", jsonSchema_post)).select(col('value.*'))
data_answer1 = data_answer.withColumn("value", from_json("value", jsonSchema_answer)).select(col('value.*'))
data_users1 = data_user.withColumn("value", from_json("value", jsonSchema_user)).select(col('value.*'))

post = data_post1.select('postid', 'CreationDate', 'OwnerUserId', 'ViewCount', 'Body')
answer = data_answer1.select('PostId', 'CreationDate','commentid', 'UserId', 'Text')

users = data_users1.select(data_users1.AccountId.cast('int').alias('accountid'), 
                                     data_users1.DisplayName.cast('string').alias('name'),
                                     data_users1.AboutMe.cast('string').alias('aboutme'),
                                     data_users1.CreationDate.cast('timestamp').alias('creationdate'),
                                     data_users1.DownVotes.cast('int').alias('downvotes'), 
                                     data_users1.UpVotes.cast('int').alias('upvotes'),
                                     data_users1.Reputation.cast('int').alias('reputation'))

post_with_ansid = post.withColumn("answerid", lit("0"))   

post_output = post_with_ansid.select(post_with_ansid.postid.cast('int'), 
                                     post_with_ansid.CreationDate.cast('timestamp').alias('creationdate'),
                                     post_with_ansid.answerid.cast('int'),
                                     post_with_ansid.OwnerUserId.cast('int').alias('userid'),
                                     post_with_ansid.Body.cast('string').alias('text'))

answer_output = answer.select(answer.PostId.cast('int').alias('postid'), 
                                     answer.CreationDate.cast('timestamp').alias('creationdate'),
                                     answer.commentid.cast('int').alias('answerid'),
                                     answer.UserId.cast('int').alias('userid'),
                                     answer.Text.cast('string').alias('text'))

def writeToCassandraTech(writeDF, epochId):
     writeDF.write \
        .format("org.apache.spark.sql.cassandra") \
        .options(table="technology", keyspace="printers")\
        .mode("append") \
        .save()

def writeToCassandraUser(writeDF, epochId):
     writeDF.write \
        .format("org.apache.spark.sql.cassandra") \
        .options(table="users", keyspace="printers")\
        .mode("append") \
        .save()

def writeToCassandraArch(writeDF, epochId):
     writeDF.write \
        .format("org.apache.spark.sql.cassandra") \
        .options(table="archive", keyspace="printers")\
        .mode("append") \
        .save()

word_list = ['abs', 'pla', 'nylon', 'petg', 'ramps', 'sunbeam', 'sanguinololu', 'arduino', 'slic3r', 
             'pronterface', 'cura']

post_tech = post.withColumn('Body', explode(split(post.Body, " "))) \
                    .withColumn('Body', lower(col('Body'))) 
post_tech = post_tech.withColumn('Body',regexp_replace(post_tech['Body'],"[^0-9a-zA-Z$]+","")) \
                         .filter(post_tech.Body.isin(word_list))
post_tech = post_tech.select(post_tech.postid.cast('int'), 
                             post_tech.Body.cast('string').alias('technology'), 
                             post_tech.CreationDate.cast('date').alias('date'), 
                             post_tech.OwnerUserId.cast('int').alias('userid'))

post_tech = post_tech.withColumn("answerid", lit("0"))
post_tech = post_tech.withColumn('date', date_format('date', 'yyyyMM'))


answer_tech = answer.withColumn('Text', explode(split(answer.Text, " "))) \
                    .withColumn('Text', lower(col('Text'))) 
answer_tech = answer_tech.withColumn('Text',regexp_replace(answer_tech['Text'],"[^0-9a-zA-Z$]+","")) \
                         .filter(answer_tech.Text.isin(word_list))
answer_tech = answer_tech.select(answer_tech.PostId.cast('int').alias('postid'), 
                                 answer_tech.commentid.cast('int').alias('answerid'), 
                                 answer_tech.Text.cast('string').alias('technology'), 
                                 answer_tech.CreationDate.cast('date').alias('date'), 
                                 answer_tech.UserId.cast('int').alias('userid'))

answer_tech = answer_tech.withColumn('date', date_format('date', 'yyyyMM'))
'''
query = answer_tech \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("append") \
    .option("truncate", "false")\
    .format("console") \
    .start()
'''
query = answer_tech \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("update") \
    .foreachBatch(writeToCassandraTech) \
    .start()

'''
query = post_tech \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("append") \
    .option("truncate", "false")\
    .format("console") \
    .start()
'''
query = post_tech \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("update") \
    .foreachBatch(writeToCassandraTech) \
    .start()
'''
query = users \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("append") \
    .option("truncate", "false")\
    .format("console") \
    .start()
'''
query = users \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("update") \
    .foreachBatch(writeToCassandraUser) \
    .start()
'''
query = post_output \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("append") \
    .option("truncate", "false")\
    .format("console") \
    .start()
'''

query = post_output \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("update") \
    .foreachBatch(writeToCassandraArch) \
    .start()

'''
query = answer_output \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("append") \
    .option("truncate", "false")\
    .format("console") \
    .start()
'''

query = answer_output \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .outputMode("update") \
    .foreachBatch(writeToCassandraArch) \
    .start()

#spark.streams.awaitAnyTermination()
query.awaitTermination()
